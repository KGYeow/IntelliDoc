{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ede8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd3abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "  text = text.lower()  # Convert to lowercase\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and non-alphanumeric characters\n",
    "  text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "  text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "  return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfbf615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "def read_document(file_path, filename):\n",
    "    # Text extraction based on file extension\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            # Use PyPDF2 for PDF text extraction\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "    elif filename.endswith(\".docx\"):\n",
    "        # Use python-docx for Word document text extraction\n",
    "        doc = Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text\n",
    "\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        # Read text directly for TXT files\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "    else:\n",
    "        # Skip unsupported file formats\n",
    "        return None\n",
    "    \n",
    "    # Preprocessing steps (clean text, lowercase, etc.)\n",
    "    text = clean_text(text) # Implement your cleaning function here\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f92b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"D:\\Documents\\USM\\USM_NotesExercises\\Year 4 Sem 1\\CAT405\\Dataset\" # Replace with your data directory path\n",
    "documents = []\n",
    "labels = []\n",
    "for class_dir in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_dir)\n",
    "    \n",
    "    if os.path.isdir(class_path):\n",
    "        for filename in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, filename)\n",
    "            text = read_document(file_path, filename)\n",
    "            \n",
    "            # Skip unsupported file formats\n",
    "            if text == None:\n",
    "                continue\n",
    "\n",
    "            documents.append(text)\n",
    "            labels.append([filename, class_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97be244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Name</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Academic_1.txt</td>\n",
       "      <td>Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Academic_10.txt</td>\n",
       "      <td>Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Academic_100.txt</td>\n",
       "      <td>Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Academic_11.txt</td>\n",
       "      <td>Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Academic_12.txt</td>\n",
       "      <td>Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>technologie_95.txt</td>\n",
       "      <td>Technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>technologie_96.txt</td>\n",
       "      <td>Technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>technologie_97.txt</td>\n",
       "      <td>Technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>technologie_98.txt</td>\n",
       "      <td>Technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>technologie_99.txt</td>\n",
       "      <td>Technical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Doc Name      Class\n",
       "0        Academic_1.txt   Academic\n",
       "1       Academic_10.txt   Academic\n",
       "2      Academic_100.txt   Academic\n",
       "3       Academic_11.txt   Academic\n",
       "4       Academic_12.txt   Academic\n",
       "..                  ...        ...\n",
       "600  technologie_95.txt  Technical\n",
       "601  technologie_96.txt  Technical\n",
       "602  technologie_97.txt  Technical\n",
       "603  technologie_98.txt  Technical\n",
       "604  technologie_99.txt  Technical\n",
       "\n",
       "[605 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(labels, columns=['Doc Name', 'Class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7353e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "y = df['Class']\n",
    "\n",
    "vectorizer_dict = {\n",
    "  \"Vocabulary\": {word: int(value) for word, value in vectorizer.vocabulary_.items()},\n",
    "  \"IDF\": vectorizer.idf_.tolist(),\n",
    "}\n",
    "with open(\"TFIDFvectorizer.json\", \"w\") as f:\n",
    "  json.dump(vectorizer_dict, f)\n",
    "joblib.dump(vectorizer, 'TFIDFvectorizer.pkl')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=800)\n",
    "trainedModel = model.fit(X_train, y_train)\n",
    "\n",
    "# dense_features = X_train.toarray()\n",
    "# model = model.fit(dense_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b1b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7355371900826446 \n",
      "\n",
      "Confusion Matrix:\n",
      "[[17  0  5  0  0  1]\n",
      " [ 1  0 17  0  0  0]\n",
      " [ 1  0 15  1  0  0]\n",
      " [ 0  1  0 14  0  0]\n",
      " [ 0  0  0  0 23  1]\n",
      " [ 1  0  0  2  1 20]] \n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Academic       0.85      0.74      0.79        23\n",
      "Administrative       0.00      0.00      0.00        18\n",
      " Co-curricular       0.41      0.88      0.56        17\n",
      "     Financial       0.82      0.93      0.87        15\n",
      "     Personnel       0.96      0.96      0.96        24\n",
      "     Technical       0.91      0.83      0.87        24\n",
      "\n",
      "      accuracy                           0.74       121\n",
      "     macro avg       0.66      0.72      0.67       121\n",
      "  weighted avg       0.69      0.74      0.70       121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = trainedModel.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred), \"\\n\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f1e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Predicted Class   : Academic\n",
      "Predicted Classes : ['Academic', 'Personnel'] \n",
      "\n",
      "~~~~~~~~~~ Predicted Probability ~~~~~~~~~~\n",
      "Predicted probability:\n",
      "Academic\t: 0.3944119074824921\n",
      "Administrative\t: 0.12258409535464634\n",
      "Co-curricular\t: 0.06557367617157046\n",
      "Financial\t: 0.12152964794976993\n",
      "Personnel\t: 0.16567812706745141\n",
      "Technical\t: 0.13022254597406965\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model for prediction\n",
    "input_file_directory = r\"D:\\Documents\\USM\\USM_NotesExercises\\Year 4 Sem 1\\CAT405\\CAT405_SRD_Report_YeowKokGuan.pdf\"\n",
    "input_file_name = input_file_directory.split(\"\\\\\")[-1]\n",
    "new_document = read_document(input_file_directory, input_file_name)\n",
    "new_features = vectorizer.transform([new_document])\n",
    "prediction = trainedModel.predict(new_features)\n",
    "prediction_proba = trainedModel.predict_proba(new_features)\n",
    "\n",
    "# Function to get predicted classes exceeding the threshold\n",
    "def get_predicted_classes(probabilities):\n",
    "    threshold = 0.15\n",
    "    class_labels = df['Class'].unique()\n",
    "    top_classes = [class_labels[i] for i, p in enumerate(probabilities[0]) if p > threshold]\n",
    "    return top_classes\n",
    "\n",
    "predicted_classes = get_predicted_classes(prediction_proba)\n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"Predicted Class   :\", prediction[0])\n",
    "print(\"Predicted Classes :\", predicted_classes, \"\\n\")\n",
    "print(\"~~~~~~~~~~ Predicted Probability ~~~~~~~~~~\")\n",
    "print(\"Predicted probability:\")\n",
    "print(\"Academic\\t:\", prediction_proba[0][0])\n",
    "print(\"Administrative\\t:\", prediction_proba[0][1])\n",
    "print(\"Co-curricular\\t:\", prediction_proba[0][2])\n",
    "print(\"Financial\\t:\", prediction_proba[0][3])\n",
    "print(\"Personnel\\t:\", prediction_proba[0][4])\n",
    "print(\"Technical\\t:\", prediction_proba[0][5])\n",
    "# print(\"Politics\\t:\", prediction_proba[0][6])\n",
    "# print(\"Space\\t\\t:\", prediction_proba[0][7])\n",
    "# print(\"Sport\\t\\t:\", prediction_proba[0][8])\n",
    "# print(\"Technologie\\t:\", prediction_proba[0][9])\n",
    "print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9143d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(model, 'DocClassificationLrModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b622acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline conversion complete!\n"
     ]
    }
   ],
   "source": [
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import convert_sklearn\n",
    "\n",
    "# Convert the model to ONNX\n",
    "# initial_type = [('features', onnx.TensorProto.FLOAT)] # Use onnx.TensorProto.FLOAT32\n",
    "initial_type = [('features', FloatTensorType([None, None]))]\n",
    "onx = convert_sklearn(trainedModel, initial_types=initial_type)\n",
    "\n",
    "# Save the converted model\n",
    "with open(\"DocClassificationLrModel.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())\n",
    "\n",
    "print(\"Pipeline conversion complete!\")\n",
    "\n",
    "# pklmodel = joblib.load(r\"D:\\Documents\\USM\\USM_NotesExercises\\Year 4 Sem 1\\CAT405\\Logistic Regression Model\\DocClassificationLrModel.pkl\")\n",
    "# onnx.save_model(pklmodel, \"DocClassificationLrModel.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
